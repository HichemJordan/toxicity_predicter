{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyjOxq5JfyR6"
   },
   "source": [
    "# Twitter connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "sdlSH5Uyfy48"
   },
   "outputs": [],
   "source": [
    "# import tweepy\n",
    "import tweepy as tw\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# your Twitter API key and API secret\n",
    "my_api_key = \"7Itm54s5C6UzzAJiHGdebyAAg\"\n",
    "my_api_secret = \"FQikM8b2O7Gsj72jiyYOkbtgIiYhfE13bjWC2EvIzUd3i4cWX5\"\n",
    "access_token = \"963549498659262466-dBbkkrXPELf24pJ5MRCUf0GC9YInnXN\"\n",
    "access_token_secret = \"KyopfN9fk1220EzPCGcalqwr00lDputoQT1ZrlyoxqmLX\"\n",
    "# authenticate\n",
    "auth = tw.OAuthHandler(my_api_key, my_api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "PDYPh9IXgAM5"
   },
   "outputs": [],
   "source": [
    "search_query = \"#harcelement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "oNvsy8fMgKY7"
   },
   "outputs": [],
   "source": [
    "# get tweets from the API\n",
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_query,\n",
    "              lang=\"fr\",\n",
    "              since=\"2008-01-01\",\n",
    "              tweet_mode=\"extended\",\n",
    "              count=1000).items(10000)\n",
    "# store the API responses in a list\n",
    "tweets_copy = []\n",
    "for tweet in tweets:\n",
    "    tweets_copy.append(tweet)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsq4_yYrgjH8",
    "outputId": "61b682e7-33df-4023-8415-47c28e0fdd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets fetched: 1222\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Tweets fetched:\", len(tweets_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-DBKJwqCEfP"
   },
   "source": [
    "# Creation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cR132k7OyCNU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# intialize the dataframe\n",
    "tweets_df = pd.DataFrame()\n",
    "# populate the dataframe\n",
    "tmp = []\n",
    "for tweet in tweets_copy:\n",
    "    hashtags = []\n",
    "    try:\n",
    "        for hashtag in tweet.entities[\"hashtags\"]:\n",
    "            hashtags.append(hashtag[\"text\"])\n",
    "        text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "    except:\n",
    "        pass\n",
    "    tmp.append(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH4peNgzIGJP"
   },
   "outputs": [],
   "source": [
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWSsaK7YCObl"
   },
   "source": [
    "# Detoxify annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bnrsm_B13Bso"
   },
   "outputs": [],
   "source": [
    "pip install detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7V3uv4P4Hj4"
   },
   "outputs": [],
   "source": [
    "from detoxify import Detoxify\n",
    "\n",
    "toxicity = []\n",
    "\n",
    "for t in tmp:\n",
    "  toxicity.append(Detoxify('multilingual').predict(t)[\"toxicity\"])\n",
    "\n",
    "print(toxicity[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J5gZnD-MSOx"
   },
   "outputs": [],
   "source": [
    "annotated = []\n",
    "for e in toxicity:\n",
    "  if e < 0.5:\n",
    "    annotated.append(1)\n",
    "  else:\n",
    "    annotated.append(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69VWvZEEOS3o"
   },
   "source": [
    "# Exportation vers csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7J3iCyOaCs-X"
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(list(zip(tmp, annotated))  ,columns = [\"tweet\",\"toxicity\"])\n",
    "dataset.to_csv(\"dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4sTWleVOZ6e"
   },
   "source": [
    "# Performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArgoynGqNZhR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from wordcloud import STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYdRF5PEP1c9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vG6soP2TrImt"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1aJtZ-oQMEF"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "total = float(len(df[\"toxicity\"]))\n",
    "\n",
    "cntplot = sns.countplot(df[\"toxicity\"])\n",
    "cntplot.set_title(' 0 for toxic and 1 for non toxic tweets')\n",
    "\n",
    "for p in ax.patches:\n",
    "    # Get height.\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVdaCYnsRNt0"
   },
   "source": [
    "# preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjPFGcYRRCTe"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"french\")\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def preprocess(text_string):\n",
    "    text_string = text_string.lower() # Convert everything to lower case.\n",
    "    text_string = re.sub('[^A-Za-z0-9]+', ' ', text_string) # Remove special characters and punctuations\n",
    "    \n",
    "    x = text_string.split()\n",
    "    new_text = []\n",
    "    \n",
    "    for word in x:\n",
    "        if word not in stop_words:\n",
    "            new_text.append(stemmer.stem(word))\n",
    "            \n",
    "    text_string = ' '.join(new_text)\n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8pYrQETRvUm"
   },
   "outputs": [],
   "source": [
    "df[\"tweet\"] = df[\"tweet\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d18mZQIFTBR9"
   },
   "outputs": [],
   "source": [
    "# Break off validation set from training data\n",
    "\n",
    "y = df.toxicity\n",
    "X = df.tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXUHj9bxW-xD"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, \n",
    "                        max_features=700, \n",
    "                        ngram_range=(1,1))\n",
    "prediction_pipeline_svm = Pipeline([('vectorizer', tfidf), \n",
    "                                ('svc_model', SVC(kernel='linear'))])\n",
    "prediction_pipeline_xgb = Pipeline([('vectorizer', tfidf), \n",
    "                                ('xgb_model', XGBRegressor(n_estimators=100, learning_rate=0.05))])\n",
    "prediction_pipeline_rfr = Pipeline([('vectorizer', tfidf), \n",
    "                                ('rfr_model', RandomForestRegressor(n_estimators=100, random_state=0))])\n",
    "\n",
    "\n",
    "prediction_pipeline_svm.fit(x_train,y_train)\n",
    "prediction_pipeline_xgb.fit(x_train,y_train)\n",
    "prediction_pipeline_rfr.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "#testing and evaluation \n",
    "\n",
    "#Make prediction on the test set\n",
    "\n",
    "pred_svm = np.round(prediction_pipeline_svm.predict(x_test))\n",
    "pred_xgb = np.round(prediction_pipeline_xgb.predict(x_test))\n",
    "pred_rfr = np.round(prediction_pipeline_rfr.predict(x_test))\n",
    "\n",
    "\n",
    "#calculating precision and recall\n",
    "\n",
    "print(\"RandomForest precision  {}, Recall  {}\".format(precision_score(list(y_test), pred_rfr) , recall_score(list(y_test), pred_rfr)))\n",
    "print(\"SVM precision  {}, Recall  {}\".format(precision_score(y_test, pred_svm) , recall_score(list(y_test), pred_svm)))\n",
    "print(\"XGBoost precision  {}, Recall  {}\".format(precision_score(y_test, pred_xgb) , recall_score(list(y_test), pred_xgb)))\n",
    "\n",
    "#F1 score\n",
    "print(\"F1 score of RandomForest model : {}\".format(f1_score(y_test, pred_rfr)))\n",
    "print(\"F1 score of SVM model : {}\".format(f1_score(y_test, pred_svm)))\n",
    "print(\"F1 score of XGBoost model : {}\".format(f1_score(y_test, pred_xgb)))\n",
    "\n",
    "#Mean absolute error\n",
    "print(\"Mean squared error of RandomForest model : {}\".format(mean_absolute_error(pred_rfr, y_test)))\n",
    "print(\"Mean squared error of SVM model : {}\".format(mean_absolute_error(pred_svm, y_test)))\n",
    "print(\"Mean squared error of XGBoost model : {}\".format(mean_absolute_error(pred_xgb, y_test)))\n",
    "\n",
    "\n",
    "#saving the model\n",
    "#xgbr.save_model(\"xgboost_model.json\")\n",
    "from joblib import dump\n",
    "dump(prediction_pipeline_svm, 'prediction_pipline_svm.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHsE6RV3cKc7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "observationDay.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
